#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì‚°ì—…ì•ˆì „ë³´ê±´ ì˜ì—­ ë²•ë ¹ PDF â†’ RAGìš© JSONL ìë™ ìƒì„±ê¸° (ë©€í‹°ë¬¸ì„œ ë°°ì¹˜ ì§€ì›)
- provisions.jsonl  : ì¡°ë¬¸(ì¥/ì ˆ/ì¡°/í•­/í˜¸/ëª©/ì„¸ëª©) ë…¸ë“œë“¤
- documents.jsonl   : ë¬¸ì„œ(ë²•ë¥ /ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™/ê¸°ì¤€/ì§€ì¹¨) ë©”íƒ€
- edges.jsonl       : (ì˜µì…˜) ìœ„ì„/ì„¸ë¶€í™”/ì¤€ìš©/ì°¸ì¡° í›„ë³´ ê´€ê³„

íŠ¹ì§•
- í•œ í´ë”ì— ì—¬ëŸ¬ ë¬¸ì„œ ê²°ê³¼ë¥¼ ì €ì¥: íŒŒì¼ëª…ì— doc_id ì ‘ë‘ì‚¬ ìë™ ë¶€ì—¬
- "ë¬¸ì„œ" ë ˆë²¨(ì„œë¬¸ ë“±)ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ
- ë°°ì¹˜ ì²˜ë¦¬: JOBS ë¦¬ìŠ¤íŠ¸ì— ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ì¶”ê°€í•´ í•œ ë²ˆì— ë³€í™˜
"""

import re
import os
import json
from dataclasses import dataclass, field, asdict
from typing import List, Optional, Dict, Tuple
from pypdf import PdfReader

# ===========================================================
# 0) ë³€í™˜í•  ë¬¸ì„œ ì‘ì—… ëª©ë¡ (ì—¬ê¸°ì— ë¬¸ì„œë¥¼ ì¶”ê°€í•˜ì„¸ìš”)
#    - doc_idëŠ” ê³ ìœ í•˜ê²Œ(ì˜ˆ: kr-osh-act/dec/rul/std/gui-...)
#    - doc_type: "ë²•ë¥ " / "ì‹œí–‰ë ¹" / "ì‹œí–‰ê·œì¹™" / "ê·œì¹™" / "ì§€ì¹¨"
# ===========================================================
JOBS = [
    # ì˜ˆì‹œ) ì‚°ì—…ì•ˆì „ë³´ê±´ë²•
    {
        "pdf": r"ì‚°ì—…ì•ˆì „ë³´ê±´ë²•_20251001.pdf",
        "doc_id": "kr-osh-act",
        "doc_type": "ë²•ë¥ ",
        "official_name_ko": "ì‚°ì—…ì•ˆì „ë³´ê±´ë²•",
        "abbrev": "ì‚°ì•ˆë²•",
        "promulgation_date": "2024-12-31",
        "enforcement_date": "2025-10-01",
        "emit_edges": True
    },
    # ì˜ˆì‹œ) ì‹œí–‰ë ¹
    {
        "pdf": r"ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì‹œí–‰ë ¹_20250621.pdf",
        "doc_id": "kr-osh-dec",
        "doc_type": "ì‹œí–‰ë ¹",
        "official_name_ko": "ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì‹œí–‰ë ¹",
        "abbrev": "ì‚°ì•ˆë²• ì‹œí–‰ë ¹",
        "promulgation_date": "2024-12-31",
        "enforcement_date": "2025-06-21",
        "emit_edges": True
    },
    # ì˜ˆì‹œ) ì‹œí–‰ê·œì¹™
    {
        "pdf": r"ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì‹œí–‰ê·œì¹™_20251001.pdf",
        "doc_id": "kr-osh-rul",
        "doc_type": "ì‹œí–‰ê·œì¹™",
        "official_name_ko": "ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì‹œí–‰ê·œì¹™",
        "abbrev": "ì‚°ì•ˆë²• ì‹œí–‰ê·œì¹™",
        "promulgation_date": "2024-12-31",
        "enforcement_date": "2025-07-01",
        "emit_edges": True
    },
    # ì˜ˆì‹œ) ê¸°ì¤€(ì‚°ì—…ì•ˆì „ë³´ê±´ê¸°ì¤€ì— ê´€í•œ ê·œì¹™)
    {
        "pdf": r"ì‚°ì—…ì•ˆì „ë³´ê±´ê¸°ì¤€ì—ê´€í•œê·œì¹™_20251001.pdf",
        "doc_id": "kr-osh-std",
        "doc_type": "ê·œì¹™",
        "official_name_ko": "ì‚°ì—…ì•ˆì „ë³´ê±´ê¸°ì¤€ì— ê´€í•œ ê·œì¹™",
        "abbrev": "ì‚°ì•ˆê¸°ì¤€ê·œì¹™",
        "promulgation_date": "2025-07-01",
        "enforcement_date": "2025-07-15",
        "emit_edges": True
    },
    # ì˜ˆì‹œ) ì•ˆì „ë³´ê±´ê³µë‹¨ ì§€ì¹¨(KOSHA Guide ë“±)
    # {
    #     "pdf": r"KOSHA_G-21-2024.pdf",
    #     "doc_id": "kr-osh-gui-g21-2024",
    #     "doc_type": "ì§€ì¹¨",
    #     "official_name_ko": "KOSHA ê°€ì´ë“œ G-21 (2024)",
    #     "abbrev": "KOSHA G-21",
    #     "promulgation_date": "2024-01-10",
    #     "enforcement_date": "2024-01-10",
    #     "emit_edges": False
    # },
]

# ê²°ê³¼ë¥¼ ì €ì¥í•  **ë‹¨ì¼ í´ë”**
OUTDIR = "./out"  # í•œ í´ë”ë¡œ í†µì¼ ì €ì¥

# ===========================================================
# 1) íŒ¨í„´ ì •ì˜
# ===========================================================
PATTERN_JANG = re.compile(r"^ì œ\s*(\d+)\s*ì¥")
PATTERN_JEOL = re.compile(r"^ì œ\s*(\d+)\s*ì ˆ")
PATTERN_JO   = re.compile(r"^ì œ\s*(\d+)\s*ì¡°")
PATTERN_HANG = re.compile(r"^([â‘ -â‘³])")
PATTERN_HO   = re.compile(r"^(\d+)\.")
PATTERN_MOK  = re.compile(r"^([ê°€-í£])\.")
PATTERN_SEMOK = re.compile(r"^(\d+)\)")

PATTERN_TITLE = re.compile(r"^(ì œ\s*\d+\s*ì¡°)\s*\(([^)]+)\)")

EDGE_RULES = [
    ("ìœ„ì„", r"ëŒ€í†µë ¹ë ¹ìœ¼ë¡œ ì •í•œë‹¤|ì‹œí–‰ë ¹ìœ¼ë¡œ ì •í•œë‹¤"),
    ("ì„¸ë¶€í™”", r"ì‹œí–‰ê·œì¹™ìœ¼ë¡œ ì •í•œë‹¤|ê·œì¹™ìœ¼ë¡œ ì •í•œë‹¤"),
    ("ì¤€ìš©", r"ì¤€ìš©í•œë‹¤"),
    ("ë‹¨ìˆœì°¸ì¡°", r"ì œ\d+ì¡°(ì˜\d+)?(ì œ\d+í•­)?(ì œ\d+í˜¸)?(ì—|ë¥¼)?\s*(ì°¸ì¡°|ë”°ë¥¸ë‹¤)")
]
EDGE_RULES_COMPILED = [(t, re.compile(p)) for t, p in EDGE_RULES]

CIRCLED_MAP = {
    "â‘ ": 1, "â‘¡": 2, "â‘¢": 3, "â‘£": 4, "â‘¤": 5, "â‘¥": 6, "â‘¦": 7, "â‘§": 8, "â‘¨": 9, "â‘©": 10,
    "â‘ª": 11, "â‘«": 12, "â‘¬": 13, "â‘­": 14, "â‘®": 15, "â‘¯": 16, "â‘°": 17, "â‘±": 18, "â‘²": 19, "â‘³": 20
}

def normalize_mok(ch: str) -> str:
    base = ord('ê°€')
    idx = ord(ch) - base
    if idx < 0 or idx > 50:
        return 'a'
    return chr(ord('a') + idx)

# ===========================================================
# 2) ë°ì´í„° êµ¬ì¡°
# ===========================================================
@dataclass
class SourceMeta:
    file_name: str
    page_range: List[int]

@dataclass
class ProvisionNode:
    id: str
    doc_id: str
    doc_type: str
    law_name_ko: str
    abbrev: Optional[str]
    title: Optional[str]
    level: str
    path_display: List[str]
    path_norm: List[str]
    label_display: Optional[str]
    label_norm: Optional[str]
    text_raw: Optional[str]
    text_clean: Optional[str]
    parent_ids: List[str]
    article_no: Optional[int]
    chapter_no: Optional[int]
    section_no: Optional[int]
    unit_index: Dict[str, object]
    effective_from: str
    effective_to: Optional[str]
    is_current: bool
    promulgation_date: Optional[str]
    enforcement_date: Optional[str]
    source: SourceMeta

    def to_json(self) -> str:
        d = asdict(self)
        d["source"] = asdict(self.source)
        return json.dumps(d, ensure_ascii=False)

@dataclass
class DocumentMeta:
    doc_id: str
    doc_type: str
    official_name_ko: str
    abbrev: Optional[str]
    issuer: Optional[str]
    promulgation_no: Optional[str]
    promulgation_date: Optional[str]
    enforcement_date: Optional[str]
    source_files: List[Dict[str, str]]

    def to_json(self) -> str:
        return json.dumps(asdict(self), ensure_ascii=False)

@dataclass
class EdgeRecord:
    edge_id: str
    edge_type: str
    from_id: str
    to_id: Optional[str]
    anchors: List[str]
    match_confidence: float

    def to_json(self) -> str:
        return json.dumps(asdict(self), ensure_ascii=False)

@dataclass
class ParseState:
    chapter_no: Optional[int] = None
    section_no: Optional[int] = None
    article_no: Optional[int] = None
    article_title: Optional[str] = None
    hang: Optional[int] = None
    ho: Optional[int] = None
    mok: Optional[str] = None
    semok: Optional[int] = None
    buffer_lines: List[str] = field(default_factory=list)
    start_page: Optional[int] = None
    end_page: Optional[int] = None

    def reset_lower(self, level: str):
        order = ["ì¥", "ì ˆ", "ì¡°", "í•­", "í˜¸", "ëª©", "ì„¸ëª©"]
        idx = order.index(level)
        for lv in order[idx:]:
            if lv == "ì¥": self.chapter_no = None
            elif lv == "ì ˆ": self.section_no = None
            elif lv == "ì¡°":
                self.article_no = None
                self.article_title = None
            elif lv == "í•­": self.hang = None
            elif lv == "í˜¸": self.ho = None
            elif lv == "ëª©": self.mok = None
            elif lv == "ì„¸ëª©": self.semok = None
        self.buffer_lines.clear()
        self.start_page = None
        self.end_page = None

# ===========================================================
# 3) í—¬í¼
# ===========================================================
def make_id(doc_id: str, state: ParseState, enforcement_date: str) -> str:
    parts = [doc_id]
    if state.article_no is not None: parts.append(str(state.article_no))
    if state.hang is not None: parts.append(str(state.hang))
    if state.ho is not None: parts.append(str(state.ho))
    if state.mok is not None: parts.append(state.mok)
    if state.semok is not None: parts.append(f"{state.semok}p")
    parts.append(enforcement_date.replace("-", ""))
    return ":".join(parts)

def display_hang(n: int) -> str:
    for k, v in CIRCLED_MAP.items():
        if v == n:
            return k
    return f"({n})"

def display_mok(a: str) -> str:
    idx = ord(a) - ord('a')
    return f"{chr(ord('ê°€') + idx)}."

def build_path_display(state: ParseState) -> List[str]:
    path = []
    if state.chapter_no: path.append(f"ì œ{state.chapter_no}ì¥")
    if state.section_no: path.append(f"ì œ{state.section_no}ì ˆ")
    if state.article_no: path.append(f"ì œ{state.article_no}ì¡°")
    if state.hang: path.append(display_hang(state.hang))
    if state.ho: path.append(f"{state.ho}.")
    if state.mok: path.append(display_mok(state.mok))
    if state.semok: path.append(f"{state.semok})")
    return path

def build_path_norm(state: ParseState) -> List[str]:
    path = []
    if state.chapter_no: path.append(f"jang:{state.chapter_no}")
    if state.section_no: path.append(f"jeol:{state.section_no}")
    if state.article_no: path.append(f"jo:{state.article_no}")
    if state.hang: path.append(f"hang:{state.hang}")
    if state.ho: path.append(f"ho:{state.ho}")
    if state.mok: path.append(f"mok:{state.mok}")
    if state.semok: path.append(f"semok:{state.semok}")
    return path

def current_level(state: ParseState) -> str:
    if state.semok is not None: return "ì„¸ëª©"
    if state.mok is not None: return "ëª©"
    if state.ho is not None: return "í˜¸"
    if state.hang is not None: return "í•­"
    if state.article_no is not None: return "ì¡°"
    if state.section_no is not None: return "ì ˆ"
    if state.chapter_no is not None: return "ì¥"
    return "ë¬¸ì„œ"

# ===========================================================
# 4) ë…¸ë“œ flush (ë¬¸ì„œ ë ˆë²¨ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ)
# ===========================================================
def flush_node(nodes: List[ProvisionNode], job: dict, state: ParseState):
    if not state.buffer_lines:
        return
    lvl = current_level(state)
    if lvl == "ë¬¸ì„œ":
        # ì„œë¬¸/ë¨¸ë¦¬ë§ ë“±ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ
        state.buffer_lines.clear()
        state.start_page = None
        state.end_page = None
        return

    label_display = label_norm = None
    if lvl == "í•­":
        label_display = display_hang(state.hang)
        label_norm = str(state.hang)
    elif lvl == "í˜¸":
        label_display = f"{state.ho}."
        label_norm = str(state.ho)
    elif lvl == "ëª©":
        label_display = display_mok(state.mok)
        label_norm = state.mok
    elif lvl == "ì„¸ëª©":
        label_display = f"{state.semok})"
        label_norm = f"{state.semok}p"

    title = f"ì œ{state.article_no}ì¡°({state.article_title})" if state.article_no and state.article_title else None
    text_raw = "\n".join(state.buffer_lines).strip()
    text_clean = re.sub(r"\s+", " ", text_raw).strip()

    # parent_ids ê°„ë‹¨ êµ¬ì„±(í•„ìš” ì‹œ ìƒì„¸ ìƒí–¥ì‹ êµ¬ì„± ì¶”ê°€ ê°€ëŠ¥)
    parent_ids: List[str] = []

    nodes.append(ProvisionNode(
        id=make_id(job["doc_id"], state, job["enforcement_date"]),
        doc_id=job["doc_id"],
        doc_type=job["doc_type"],
        law_name_ko=job["official_name_ko"],
        abbrev=job.get("abbrev"),
        title=title,
        level=lvl,
        path_display=build_path_display(state),
        path_norm=build_path_norm(state),
        label_display=label_display,
        label_norm=label_norm,
        text_raw=text_raw,
        text_clean=text_clean,
        parent_ids=parent_ids,
        article_no=state.article_no,
        chapter_no=state.chapter_no,
        section_no=state.section_no,
        unit_index={"hang": state.hang, "ho": state.ho, "mok": state.mok, "semok": state.semok},
        effective_from=job["enforcement_date"],
        effective_to=None,
        is_current=True,
        promulgation_date=job.get("promulgation_date"),
        enforcement_date=job.get("enforcement_date"),
        source=SourceMeta(
            file_name=os.path.basename(job["pdf"]),
            page_range=[(state.start_page or 0) + 1, (state.end_page or 0) + 1]
        )
    ))
    state.buffer_lines.clear()
    state.start_page = None
    state.end_page = None

# ===========================================================
# 5) PDF íŒŒì„œ
# ===========================================================
def parse_pdf(job: dict) -> Tuple[List[ProvisionNode], List[EdgeRecord]]:
    reader = PdfReader(job["pdf"])
    nodes: List[ProvisionNode] = []
    edges: List[EdgeRecord] = []
    state = ParseState()
    edge_counter = 0

    def update_title(line: str):
        m = PATTERN_TITLE.search(line)
        if m:
            state.article_title = m.group(2).strip()

    for pidx, page in enumerate(reader.pages):
        text = page.extract_text() or ""
        for line in text.splitlines():
            s = line.strip()
            if not s:
                continue

            # ê³„ì¸µ íƒì§€
            m = PATTERN_JANG.match(s)
            if m:
                flush_node(nodes, job, state)
                state.reset_lower("ì¥")
                state.chapter_no = int(m.group(1))
                continue

            m = PATTERN_JEOL.match(s)
            if m:
                flush_node(nodes, job, state)
                state.reset_lower("ì ˆ")
                state.section_no = int(m.group(1))
                continue

            m = PATTERN_JO.match(s)
            if m:
                flush_node(nodes, job, state)
                state.reset_lower("ì¡°")
                state.article_no = int(m.group(1))
                update_title(s)
                # ì¡° ì œëª© ë¼ì¸ ì´í›„ ì‹¤ì œ ë³¸ë¬¸ì€ í•­/í˜¸/ëª© ë“±ì—ì„œ ìˆ˜ì§‘
                continue

            m = PATTERN_HANG.match(s)
            if m:
                flush_node(nodes, job, state)
                state.reset_lower("í•­")
                state.hang = CIRCLED_MAP.get(m.group(1))
                s = s[len(m.group(1)):].strip()
            else:
                m = PATTERN_HO.match(s)
                if m and state.article_no is not None:
                    flush_node(nodes, job, state)
                    state.reset_lower("í˜¸")
                    state.ho = int(m.group(1))
                    s = s[m.end():].strip()
                else:
                    m = PATTERN_MOK.match(s)
                    if m and state.article_no is not None:
                        flush_node(nodes, job, state)
                        state.reset_lower("ëª©")
                        state.mok = normalize_mok(m.group(1))
                        s = s[m.end():].strip()
                    else:
                        m = PATTERN_SEMOK.match(s)
                        if m and state.article_no is not None:
                            flush_node(nodes, job, state)
                            state.reset_lower("ì„¸ëª©")
                            state.semok = int(m.group(1))
                            s = s[m.end():].strip()

            # ë³¸ë¬¸ ì¶•ì 
            if s:
                if state.start_page is None:
                    state.start_page = pidx
                state.end_page = pidx
                state.buffer_lines.append(s)
                if state.article_no:
                    update_title(s)

                # ê´€ê³„(ì—ì§€) í›„ë³´ ì¶”ì¶œ
                if job.get("emit_edges"):
                    for etype, rx in EDGE_RULES_COMPILED:
                        if rx.search(s):
                            edge_counter += 1
                            edges.append(EdgeRecord(
                                edge_id=f"e-{edge_counter:06d}",
                                edge_type=etype,
                                from_id=make_id(job["doc_id"], state, job["enforcement_date"]),
                                to_id=None,  # í›„ì²˜ë¦¬ ë§¤í•‘ ë‹¨ê³„ì—ì„œ ì—°ê²° ê¶Œì¥
                                anchors=[s[:120]],
                                match_confidence=0.70 if etype != "ë‹¨ìˆœì°¸ì¡°" else 0.55
                            ))

    # ë§ˆì§€ë§‰ ë²„í¼ flush
    flush_node(nodes, job, state)
    return nodes, edges

# ===========================================================
# 6) JSONL ì“°ê¸°
# ===========================================================
def write_jsonl(path: str, rows: List[str]):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        for r in rows:
            f.write(r)
            f.write("\n")

def build_document_meta(job: dict) -> DocumentMeta:
    return DocumentMeta(
        doc_id=job["doc_id"],
        doc_type=job["doc_type"],
        official_name_ko=job["official_name_ko"],
        abbrev=job.get("abbrev"),
        issuer=None,
        promulgation_no=None,
        promulgation_date=job.get("promulgation_date"),
        enforcement_date=job.get("enforcement_date"),
        source_files=[{"file_name": os.path.basename(job["pdf"])}]
    )

# ===========================================================
# 7) ì‹¤í–‰ë¶€ (ë°°ì¹˜ ì²˜ë¦¬)
# ===========================================================
def run_all(jobs: List[dict], outdir: str):
    os.makedirs(outdir, exist_ok=True)
    for job in jobs:
        # í•„ìˆ˜ í™•ì¸
        required = ["pdf","doc_id","doc_type","official_name_ko","enforcement_date"]
        missing = [k for k in required if not job.get(k)]
        if missing:
            print(f"âŒ ëˆ„ë½ëœ ì„¤ì •: {missing} (doc_id={job.get('doc_id')})")
            continue

        pdf_path = job["pdf"]
        if not os.path.exists(pdf_path):
            print(f"âŒ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pdf_path} (doc_id={job['doc_id']})")
            continue

        print(f"\nğŸ“˜ ì²˜ë¦¬ ì‹œì‘: {job['official_name_ko']} (doc_id={job['doc_id']})")
        print(f"    PDF: {pdf_path}")
        nodes, edges = parse_pdf(job)

        # íŒŒì¼ëª…ì— doc_id ì ‘ë‘ì‚¬ ë¶€ì—¬ â†’ í•œ í´ë”ì— ëª¨ë‘ ì €ì¥ ê°€ëŠ¥
        prov_path = os.path.join(outdir, f"{job['doc_id']}_provisions.jsonl")
        doc_path  = os.path.join(outdir, f"{job['doc_id']}_documents.jsonl")
        edge_path = os.path.join(outdir, f"{job['doc_id']}_edges.jsonl")

        write_jsonl(prov_path, [n.to_json() for n in nodes])
        write_jsonl(doc_path,  [build_document_meta(job).to_json()])
        if job.get("emit_edges") and edges:
            write_jsonl(edge_path, [e.to_json() for e in edges])

        print(f"   [OK] {prov_path}")
        print(f"   [OK] {doc_path}")
        if job.get("emit_edges") and edges:
            print(f"   [OK] {edge_path}")
        else:
            print(f"   [i ] edges ë¯¸ìƒì„±(emit_edges={job.get('emit_edges')}, í›„ë³´={len(edges)})")

if __name__ == "__main__":
    print(f"ğŸ—‚ ì¶œë ¥ í´ë”: {os.path.abspath(OUTDIR)}")
    run_all(JOBS, OUTDIR)
    print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ")
