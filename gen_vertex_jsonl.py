#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PDF(êµ­ë‚´ ë²•ë ¹) â†’ Vertex AI Search JSONL (Structured / JSONL with document IDs)

- ì¶œë ¥: ë¼ì¸ë‹¹ í•œ ê°œ JSON ê°ì²´ (top-levelì€ id + structData ë§Œ ì‚¬ìš©)
  {
    "id": "ë¬¸ì„œID(ì˜ë¬¸/ìˆ«ì/_/- ë§Œ)",
    "structData": {
      "title": "...",          # í‘œì‹œ ì œëª©
      "text": "...",           # ê²€ìƒ‰ ë³¸ë¬¸
      "uri": "...",            # ì›ë¬¸ ê²½ë¡œ(ì„ íƒ)
      "law_name_ko": "...",    # ì´í•˜ ë©”íƒ€ (í•„í„°/íŒŒì‹¯ì— í™œìš©)
      "doc_type": "...",
      "level": "...",          # ì¥/ì ˆ/ì¡°/í•­/í˜¸/ëª©/ì„¸ëª©
      "path_display": "...",
      "version_date": "YYYY-MM-DD",
      "doc_id": "...",
      "abbrev": "...",
      "path_norm": ["jang:1","jo:2","hang:1", ...],
      "article_no": 1,
      "chapter_no": 1,
      "section_no": 1,
      "effective_from": "YYYY-MM-DD",
      "promulgation_date": "YYYY-MM-DD",
      "enforcement_date": "YYYY-MM-DD",
      "source_file": "íŒŒì¼ëª….pdf",
      "page_range": [ì‹œì‘, ë]  # 1-based
    }
  }

- ì˜ì¡´: pypdf  (pip install pypdf)
- ì„œë¬¸(â€œë¬¸ì„œâ€ ë ˆë²¨)ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ
"""

import os
import re
import json
import hashlib
from dataclasses import dataclass, field
from typing import List, Optional, Dict
from pypdf import PdfReader

# ===========================================================
# 0) ë³€í™˜í•  ë¬¸ì„œ ì‘ì—… ëª©ë¡
# ===========================================================
JOBS = [
    {
        "pdf": r"ì‚°ì—…ì•ˆì „ë³´ê±´ë²•_20251001.pdf",
        "doc_id": "kr-osh-act",
        "doc_type": "ë²•ë¥ ",
        "official_name_ko": "ì‚°ì—…ì•ˆì „ë³´ê±´ë²•",
        "abbrev": "ì‚°ì•ˆë²•",
        "promulgation_date": "2024-12-31",
        "enforcement_date": "2025-10-01",
        # "uri": "gs://your-bucket/laws/ì‚°ì—…ì•ˆì „ë³´ê±´ë²•_20251001.pdf",
    },
    {
        "pdf": r"ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì‹œí–‰ë ¹_20250621.pdf",
        "doc_id": "kr-osh-dec",
        "doc_type": "ì‹œí–‰ë ¹",
        "official_name_ko": "ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì‹œí–‰ë ¹",
        "abbrev": "ì‚°ì•ˆë²• ì‹œí–‰ë ¹",
        "promulgation_date": "2024-12-31",
        "enforcement_date": "2025-06-21",
    },
    {
        "pdf": r"ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì‹œí–‰ê·œì¹™_20251001.pdf",
        "doc_id": "kr-osh-rul",
        "doc_type": "ì‹œí–‰ê·œì¹™",
        "official_name_ko": "ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì‹œí–‰ê·œì¹™",
        "abbrev": "ì‚°ì•ˆë²• ì‹œí–‰ê·œì¹™",
        "promulgation_date": "2024-12-31",
        "enforcement_date": "2025-07-01",
    },
    {
        "pdf": r"ì‚°ì—…ì•ˆì „ë³´ê±´ê¸°ì¤€ì—ê´€í•œê·œì¹™_20251001.pdf",
        "doc_id": "kr-osh-std",
        "doc_type": "ê·œì¹™",
        "official_name_ko": "ì‚°ì—…ì•ˆì „ë³´ê±´ê¸°ì¤€ì— ê´€í•œ ê·œì¹™",
        "abbrev": "ì‚°ì•ˆê¸°ì¤€ê·œì¹™",
        "promulgation_date": "2025-07-01",
        "enforcement_date": "2025-07-15",
    },
]

OUTDIR = "./out_vertex"

# ===========================================================
# 1) íŒ¨í„´ (ì¥-ì ˆ-ì¡°-í•­-í˜¸-ëª©-ì„¸ëª©)
# ===========================================================
PATTERN_JANG = re.compile(r"^ì œ\s*(\d+)\s*ì¥")
PATTERN_JEOL = re.compile(r"^ì œ\s*(\d+)\s*ì ˆ")
PATTERN_JO   = re.compile(r"^ì œ\s*(\d+)\s*ì¡°")
PATTERN_HANG = re.compile(r"^([â‘ -â‘³])")
PATTERN_HO   = re.compile(r"^(\d+)\.")
PATTERN_MOK  = re.compile(r"^([ê°€-í£])\.")
PATTERN_SEMOK= re.compile(r"^(\d+)\)")
PATTERN_TITLE= re.compile(r"^(ì œ\s*\d+\s*ì¡°)\s*\(([^)]+)\)")

CIRCLED_MAP = {
    "â‘ ": 1, "â‘¡": 2, "â‘¢": 3, "â‘£": 4, "â‘¤": 5, "â‘¥": 6, "â‘¦": 7, "â‘§": 8, "â‘¨": 9, "â‘©": 10,
    "â‘ª": 11, "â‘«": 12, "â‘¬": 13, "â‘­": 14, "â‘®": 15, "â‘¯": 16, "â‘°": 17, "â‘±": 18, "â‘²": 19, "â‘³": 20
}

def normalize_mok(ch: str) -> str:
    base = ord('ê°€')
    idx = ord(ch) - base
    if idx < 0 or idx > 50:
        return 'a'
    return chr(ord('a') + idx)

# ===========================================================
# 2) íŒŒì‹± ìƒíƒœ
# ===========================================================
@dataclass
class ParseState:
    chapter_no: Optional[int] = None
    section_no: Optional[int] = None
    article_no: Optional[int] = None
    article_title: Optional[str] = None
    hang: Optional[int] = None
    ho: Optional[int] = None
    mok: Optional[str] = None
    semok: Optional[int] = None
    buffer_lines: List[str] = field(default_factory=list)
    start_page: Optional[int] = None
    end_page: Optional[int] = None

    def reset_lower(self, level: str):
        order = ["ì¥", "ì ˆ", "ì¡°", "í•­", "í˜¸", "ëª©", "ì„¸ëª©"]
        idx = order.index(level)
        for lv in order[idx:]:
            if lv == "ì¥": self.chapter_no = None
            elif lv == "ì ˆ": self.section_no = None
            elif lv == "ì¡°":
                self.article_no = None
                self.article_title = None
            elif lv == "í•­": self.hang = None
            elif lv == "í˜¸": self.ho = None
            elif lv == "ëª©": self.mok = None
            elif lv == "ì„¸ëª©": self.semok = None
        self.buffer_lines.clear()
        self.start_page = None
        self.end_page = None

# ===========================================================
# 3) ë„ìš°ë¯¸
# ===========================================================
def display_hang(n: int) -> str:
    for k, v in CIRCLED_MAP.items():
        if v == n:
            return k
    return f"({n})"

def display_mok(a: str) -> str:
    idx = ord(a) - ord('a')
    return f"{chr(ord('ê°€') + idx)}."

def build_path_display(state: ParseState) -> List[str]:
    path = []
    if state.chapter_no: path.append(f"ì œ{state.chapter_no}ì¥")
    if state.section_no: path.append(f"ì œ{state.section_no}ì ˆ")
    if state.article_no: path.append(f"ì œ{state.article_no}ì¡°")
    if state.hang: path.append(display_hang(state.hang))
    if state.ho: path.append(f"{state.ho}.")
    if state.mok: path.append(display_mok(state.mok))
    if state.semok: path.append(f"{state.semok})")
    return path

def build_path_norm(state: ParseState) -> List[str]:
    path = []
    if state.chapter_no: path.append(f"jang:{state.chapter_no}")
    if state.section_no: path.append(f"jeol:{state.section_no}")
    if state.article_no: path.append(f"jo:{state.article_no}")
    if state.hang: path.append(f"hang:{state.hang}")
    if state.ho: path.append(f"ho:{state.ho}")
    if state.mok: path.append(f"mok:{state.mok}")
    if state.semok: path.append(f"semok:{state.semok}")
    return path

def current_level(state: ParseState) -> str:
    if state.semok is not None: return "ì„¸ëª©"
    if state.mok is not None: return "ëª©"
    if state.ho is not None: return "í˜¸"
    if state.hang is not None: return "í•­"
    if state.article_no is not None: return "ì¡°"
    if state.section_no is not None: return "ì ˆ"
    if state.chapter_no is not None: return "ì¥"
    return "ë¬¸ì„œ"

def build_vertex_title(job: Dict, state: ParseState) -> str:
    if state.article_no:
        if state.article_title:
            return f"{job['official_name_ko']} ì œ{state.article_no}ì¡°({state.article_title})" + \
                   (" " + display_hang(state.hang) if state.hang else "")
        base = f"{job['official_name_ko']} ì œ{state.article_no}ì¡°"
        return base + (f" {display_hang(state.hang)}" if state.hang else "")
    pd = " > ".join(build_path_display(state))
    return f"{job['official_name_ko']} {pd}" if pd else job['official_name_ko']

# ---- ì•ˆì „í•œ ë¬¸ì„œ ID ìƒì„± (ì˜ë¬¸/ìˆ«ì/_/- ë§Œ, ê¸¸ì´ ì œí•œ) -------------------------
def make_id(doc_id: str, state: ParseState, enforcement_date: str) -> str:
    parts = [doc_id]
    if state.article_no is not None: parts.append(str(state.article_no))
    if state.hang  is not None: parts.append(str(state.hang))
    if state.ho    is not None: parts.append(str(state.ho))
    if state.mok   is not None: parts.append(state.mok)
    if state.semok is not None: parts.append(f"{state.semok}p")
    parts.append(enforcement_date.replace("-", ""))  # YYYYMMDD

    raw = "_".join(parts)
    safe = re.sub(r"[^A-Za-z0-9_-]", "_", raw)      # í—ˆìš©ë¬¸ìë§Œ
    safe = re.sub(r"_+", "_", safe).strip("_")

    MAX_LEN = 120
    if len(safe) > MAX_LEN:
        h = hashlib.sha1(safe.encode("utf-8")).hexdigest()[:8]
        safe = (safe[:MAX_LEN-9] + "_" + h).strip("_")

    return safe or "doc_" + hashlib.sha1(raw.encode("utf-8")).hexdigest()[:8]

# ===========================================================
# 4) PDF â†’ JSONL ë¼ì¸
# ===========================================================
def parse_pdf_to_vertex_lines(job: Dict) -> List[str]:
    reader = PdfReader(job["pdf"])
    out_lines: List[str] = []
    state = ParseState()

    def flush_vertex_record():
        if not state.buffer_lines:
            return
        lvl = current_level(state)
        if lvl == "ë¬¸ì„œ":
            state.buffer_lines.clear(); state.start_page = None; state.end_page = None
            return

        content = re.sub(r"\s+", " ", "\n".join(state.buffer_lines).strip())
        if not content:
            state.buffer_lines.clear(); state.start_page = None; state.end_page = None
            return

        vid = make_id(job["doc_id"], state, job["enforcement_date"])
        title = build_vertex_title(job, state)
        path_disp = " > ".join(build_path_display(state))

        record = {
            "id": vid,
            "structData": {
                "title": title,
                "text": content,
                "uri": job.get("uri") or os.path.basename(job["pdf"]),
                "law_name_ko": job["official_name_ko"],
                "doc_type": job["doc_type"],
                "level": lvl,
                "path_display": path_disp,
                "version_date": job.get("enforcement_date"),
                "doc_id": job["doc_id"],
                "abbrev": job.get("abbrev"),
                "path_norm": build_path_norm(state),
                "article_no": state.article_no,
                "chapter_no": state.chapter_no,
                "section_no": state.section_no,
                "effective_from": job.get("enforcement_date"),
                "promulgation_date": job.get("promulgation_date"),
                "enforcement_date": job.get("enforcement_date"),
                "source_file": os.path.basename(job["pdf"]),
                "page_range": [ (state.start_page or 0) + 1, (state.end_page or 0) + 1 ],
            }
        }
        out_lines.append(json.dumps(record, ensure_ascii=False))
        state.buffer_lines.clear(); state.start_page = None; state.end_page = None

    def update_title_from_line(line: str):
        m = PATTERN_TITLE.search(line)
        if m:
            state.article_title = m.group(2).strip()

    # í˜ì´ì§€ ìˆœíšŒ
    for pidx, page in enumerate(reader.pages):
        text = page.extract_text() or ""
        for line in text.splitlines():
            s = line.strip()
            if not s:
                continue

            m = PATTERN_JANG.match(s)
            if m:
                flush_vertex_record(); state.reset_lower("ì¥")
                state.chapter_no = int(m.group(1)); continue

            m = PATTERN_JEOL.match(s)
            if m:
                flush_vertex_record(); state.reset_lower("ì ˆ")
                state.section_no = int(m.group(1)); continue

            m = PATTERN_JO.match(s)
            if m:
                flush_vertex_record(); state.reset_lower("ì¡°")
                state.article_no = int(m.group(1)); update_title_from_line(s); continue

            m = PATTERN_HANG.match(s)
            if m:
                flush_vertex_record(); state.reset_lower("í•­")
                state.hang = CIRCLED_MAP.get(m.group(1)); s = s[len(m.group(1)):].strip()
            else:
                m = PATTERN_HO.match(s)
                if m and state.article_no is not None:
                    flush_vertex_record(); state.reset_lower("í˜¸")
                    state.ho = int(m.group(1)); s = s[m.end():].strip()
                else:
                    m = PATTERN_MOK.match(s)
                    if m and state.article_no is not None:
                        flush_vertex_record(); state.reset_lower("ëª©")
                        state.mok = normalize_mok(m.group(1)); s = s[m.end():].strip()
                    else:
                        m = PATTERN_SEMOK.match(s)
                        if m and state.article_no is not None:
                            flush_vertex_record(); state.reset_lower("ì„¸ëª©")
                            state.semok = int(m.group(1)); s = s[m.end():].strip()

            # ë³¸ë¬¸ ì¶•ì 
            if s:
                if state.start_page is None: state.start_page = pidx
                state.end_page = pidx
                state.buffer_lines.append(s)
                if state.article_no: update_title_from_line(s)

    flush_vertex_record()
    return out_lines

# ===========================================================
# 5) ì“°ê¸°/ì‹¤í–‰
# ===========================================================
def write_lines(path: str, lines: List[str]):
    d = os.path.dirname(path)
    if d: os.makedirs(d, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        for ln in lines:
            f.write(ln + "\n")

def run_all(jobs: List[Dict], outdir: str):
    os.makedirs(outdir, exist_ok=True)
    for job in jobs:
        required = ["pdf","doc_id","doc_type","official_name_ko","enforcement_date"]
        missing = [k for k in required if not job.get(k)]
        if missing:
            print(f"âŒ ëˆ„ë½ëœ ì„¤ì •: {missing} (doc_id={job.get('doc_id')})"); continue
        if not os.path.exists(job["pdf"]):
            print(f"âŒ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {job['pdf']} (doc_id={job['doc_id']})"); continue

        print(f"\nğŸ“˜ ì²˜ë¦¬: {job['official_name_ko']} (doc_id={job['doc_id']})")
        lines = parse_pdf_to_vertex_lines(job)
        out_path = os.path.join(outdir, f"vertex_{job['doc_id']}.jsonl")
        write_lines(out_path, lines)
        print(f"   [OK] {out_path}  (records={len(lines)})")

if __name__ == "__main__":
    print(f"ğŸ—‚ ì¶œë ¥ í´ë”: {os.path.abspath(OUTDIR)}")
    run_all(JOBS, OUTDIR)
    print("\nâœ… ì™„ë£Œ")



